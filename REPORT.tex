\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}

\title{\textbf{Autoencoder Implementation Report\\FFHQ Dataset}}
\author{Ameer George Abdallah}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}

This report presents the implementation and evaluation of multiple autoencoder architectures trained on the FFHQ (Flickr-Faces-HQ) dataset. The goal is to achieve high-quality image reconstruction while maintaining an informative latent space representation.

\subsection{Dataset}
\begin{itemize}
    \item \textbf{Dataset}: FFHQ (Flickr-Faces-HQ)
    \item \textbf{Location}: \texttt{/home/ML\_courses/03683533\_2025/dataset}
    \item \textbf{Image Size}: 256×256 pixels
    \item \textbf{Training Set}: 39,000 images
    \item \textbf{Validation Set}: 1,000 images (first 1K)
\end{itemize}

\subsection{Evaluation Metrics}
\begin{itemize}
    \item \textbf{L1 Loss}: Mean Absolute Error between reconstruction and input
    \item \textbf{PSNR}: Peak Signal-to-Noise Ratio (dB)
    \item \textbf{LPIPS}: Learned Perceptual Image Patch Similarity (when applicable)
    \item \textbf{KL Divergence}: For VAE models, measures latent space regularization
\end{itemize}

\newpage

\section{Part A: Skip-Connection Autoencoders}

This section explores U-Net style autoencoders with skip connections, testing different normalization techniques, activations, and network widths.

\subsection{Model A1: Skip ConvAE (Depth 10)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{SkipConvAutoencoder}
    \item \textbf{Base Channels}: 32
    \item \textbf{Number of Blocks}: 4
    \item \textbf{Latent Dimension}: 256 (flattened)
    \item \textbf{Normalization}: GroupNorm
    \item \textbf{Activation}: SiLU (Swish)
    \item \textbf{Output Activation}: Tanh
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 60 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model A1}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_A1_skip_convae_depth10.png}
\caption{Training and validation loss curves for Model A1}
\end{figure}

\newpage

\subsection{Model A2: Skip ConvAE (BatchNorm)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{SkipConvAutoencoder}
    \item \textbf{Base Channels}: 32
    \item \textbf{Number of Blocks}: 4
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Normalization}: BatchNorm
    \item \textbf{Activation}: SiLU
    \item \textbf{Output Activation}: Tanh
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 60 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model A2}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_A2_skip_convae_bn.png}
\caption{Training and validation loss curves for Model A2}
\end{figure}

\newpage

\subsection{Model A3: Skip ConvAE (LeakyReLU)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{SkipConvAutoencoder}
    \item \textbf{Base Channels}: 32
    \item \textbf{Number of Blocks}: 4
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Normalization}: GroupNorm
    \item \textbf{Activation}: LeakyReLU (slope=0.2)
    \item \textbf{Output Activation}: Tanh
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 60 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model A3}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_A3_skip_convae_leaky.png}
\caption{Training and validation loss curves for Model A3}
\end{figure}

\newpage

\subsection{Model A4: Skip ConvAE (Wide)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{SkipConvAutoencoder}
    \item \textbf{Base Channels}: 48 (wider network)
    \item \textbf{Number of Blocks}: 4
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Normalization}: GroupNorm
    \item \textbf{Activation}: SiLU
    \item \textbf{Output Activation}: Tanh
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 60 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model A4}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_A4_skip_convae_wide.png}
\caption{Training and validation loss curves for Model A4}
\end{figure}

\newpage

\section{Part B: Deep ResNet Autoencoders with LPIPS}

This section explores deep ResNet-based autoencoders with varying LPIPS (Learned Perceptual Image Patch Similarity) weights to improve perceptual quality.

\subsection{Model B1: ResNet AE (Baseline)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{DeepResNetAutoencoder}
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Encoder}: 5 downsampling blocks with ResBlocks
    \item \textbf{Decoder}: 5 upsampling blocks with ResBlocks
    \item \textbf{Normalization}: BatchNorm
    \item \textbf{Activation}: LeakyReLU (0.2)
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
LPIPS Weight & 0.0 (None) \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model B1}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_B1_resnet_ae.png}
\caption{Training and validation loss curves for Model B1}
\end{figure}

\newpage

\subsection{Model B2: ResNet + LPIPS Ablation Study}

We conducted an ablation study on LPIPS weight to determine its effect on reconstruction quality.

\subsubsection{Model B2.1: LPIPS Weight = 0.0}

\paragraph{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
LPIPS Weight & 0.0 \\
LPIPS Network & AlexNet \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_B2_resnet_lpips_w000.png}
\caption{Loss curves for Model B2.1 (LPIPS weight = 0.0)}
\end{figure}

\newpage

\subsubsection{Model B2.2: LPIPS Weight = 0.05}

\paragraph{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + LPIPS \\
LPIPS Weight & 0.05 \\
LPIPS Network & AlexNet \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_B2_resnet_lpips_w005.png}
\caption{Loss curves for Model B2.2 (LPIPS weight = 0.05)}
\end{figure}

\newpage

\subsubsection{Model B2.3: LPIPS Weight = 0.075}

\paragraph{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + LPIPS \\
LPIPS Weight & 0.075 \\
LPIPS Network & AlexNet \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_B2_resnet_lpips_w0075.png}
\caption{Loss curves for Model B2.3 (LPIPS weight = 0.075)}
\end{figure}

\newpage

\subsubsection{Model B2.4: LPIPS Weight = 0.10}

\paragraph{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + LPIPS \\
LPIPS Weight & 0.10 \\
LPIPS Network & AlexNet \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_B2_resnet_lpips_w010.png}
\caption{Loss curves for Model B2.4 (LPIPS weight = 0.10)}
\end{figure}

\newpage

\section{Part C: Variational Autoencoders (VAE)}

This section explores VAE models with different KL divergence weights and architectural variations to balance reconstruction quality with latent space regularization.

\subsection{Model C1: VAE (Small Beta)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{VAE}
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Base Channels}: 32
    \item \textbf{Encoder}: 4 convolutional layers with BatchNorm
    \item \textbf{Decoder}: 4 transposed convolutional layers
    \item \textbf{Activation}: LeakyReLU (0.2)
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 20 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence \\
KLD Weight (β) & 0.00025 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model C1}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_C1_vae_beta_small.png}
\caption{Training and validation loss curves for Model C1}
\end{figure}

\newpage

\subsection{Model C2: VAE (Medium Beta)}

\subsubsection{Architecture}
Same as Model C1.

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 20 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence \\
KLD Weight (β) & 0.001 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model C2}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_C2_vae_beta_medium.png}
\caption{Training and validation loss curves for Model C2}
\end{figure}

\newpage

\subsection{Model C3: VAE (Large Beta)}

\subsubsection{Architecture}
Same as Model C1.

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 20 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence \\
KLD Weight (β) & 0.01 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model C3}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_C3_vae_beta_large.png}
\caption{Training and validation loss curves for Model C3}
\end{figure}

\newpage

\subsection{Model C4: VAE (Wide Network)}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{VAE}
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Base Channels}: 64 (wider network)
    \item \textbf{Encoder}: 4 convolutional layers
    \item \textbf{Decoder}: 4 transposed convolutional layers
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 20 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence \\
KLD Weight (β) & 0.00025 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model C4}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_C4_vae_wide.png}
\caption{Training and validation loss curves for Model C4}
\end{figure}

\newpage

\subsection{Model C5: VAE + LPIPS}

\subsubsection{Architecture}
Same as Model C1 (base channels = 32).

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 20 \\
Batch Size & 8 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence + LPIPS \\
KLD Weight (β) & 0.00025 \\
LPIPS Weight & 0.1 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model C5}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_C5_vae_lpips.png}
\caption{Training and validation loss curves for Model C5}
\end{figure}

\newpage

\section{Part D: Deep VAE with Attention}

This section presents advanced models combining deep ResNet architectures with self-attention mechanisms and VAE regularization.

\subsection{Model D1: Deep ResNet VAE}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{DeepResNetVAE}
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Encoder}: 5 downsampling blocks with ResBlocks
    \item \textbf{Decoder}: 5 upsampling blocks with ResBlocks
    \item \textbf{Normalization}: BatchNorm
    \item \textbf{Activation}: LeakyReLU (0.2)
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 4 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence \\
KLD Weight (β) & 0.00025 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model D1}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_D1_deep_resnet_vae.png}
\caption{Training and validation loss curves for Model D1}
\end{figure}

\newpage

\subsection{Model D2: Deep ResNet VAE + LPIPS}

\subsubsection{Architecture}
Same as Model D1.

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 4 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 + KL Divergence + LPIPS \\
KLD Weight (β) & 0.00025 \\
LPIPS Weight & 0.1 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model D2}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_D2_deep_resnet_vae_lpips.png}
\caption{Training and validation loss curves for Model D2}
\end{figure}

\newpage

\subsection{Model D3: Deep ResNet AE + Attention}

\subsubsection{Architecture}
\begin{itemize}
    \item \textbf{Model Class}: \texttt{DeepResNetAttentionAE}
    \item \textbf{Latent Dimension}: 256
    \item \textbf{Encoder}: 5 downsampling blocks with ResBlocks
    \item \textbf{Decoder}: 5 upsampling blocks with ResBlocks
    \item \textbf{Attention}: Multi-head self-attention at 16×16 and 32×32 resolutions
    \item \textbf{Attention Heads}: 8
    \item \textbf{Normalization}: GroupNorm
\end{itemize}

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Epochs & 30 \\
Batch Size & 4 \\
Learning Rate & 2e-4 \\
Optimizer & Adam \\
Loss Function & L1 Loss \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for Model D3}
\end{table}

\subsubsection{Loss Curves}
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../report_assets/loss_curves/run_D3_attention_ae.png}
\caption{Training and validation loss curves for Model D3}
\end{figure}

\newpage

\section{Conclusion}

This report presented a comprehensive study of 17 different autoencoder architectures trained on the FFHQ dataset. The models explored various design choices including:

\begin{itemize}
    \item Skip connections vs. bottleneck architectures
    \item Different normalization techniques (BatchNorm vs. GroupNorm)
    \item Activation functions (SiLU, LeakyReLU)
    \item Perceptual loss (LPIPS) with varying weights
    \item VAE regularization with different KL divergence weights
    \item Self-attention mechanisms for capturing global dependencies
\end{itemize}

The loss curves demonstrate the training dynamics and convergence behavior of each model, providing insights into the effectiveness of different architectural choices and hyperparameter settings.

\end{document}
